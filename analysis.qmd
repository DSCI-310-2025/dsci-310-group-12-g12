---
title: "Predicting Credit Card Default Behavior"
author: "Daniel Park | Yiting Sun | Jellia Ma | Han Qin"
format:
  html:
    toc: true
    number-sections: true
bibliography: references.bib
editor: source
---

::: {#fig-intro-image}
![Images of Credit Cards [@radio_canada_nodate]](analysis_files/credit_card_image.png)
:::

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)

suppressPackageStartupMessages({
  library(creditCardTool) 
  library(tidyverse)       
  library(caret)           
  library(class)          
  library(knitr)           
})

```

# Summary

In this study, we developed a classification model to predict whether credit card customers will default based on their demographic features, financial behaviors, and payment history. 
Using the Default of Credit Card Clients Dataset [@uci2016], we analyzed key factors that contribute to default risk.


Our goal was to build a predictive model that can assist financial institutions in assessing credit risk more effectively. 
The k-nearest neighbors (KNN) algorithm was implemented to determine whether a customer will default in the next month. 
The model was trained on historical data, incorporating variables such as given credit limit, repayment status, history payments amount, and demographic characteristics. 
We evaluated its performance using confusion matrix. The KNN classification model achieved an accuracy of approximately 80%, with high sensitivity but low specificity, 
indicating that while the model effectively identifies non-defaulters, it struggles with correctly predicting actual defaulters.

# Introduction

By 2023, approximately 1.25 billion people worldwide have credit cards, which is nearly 16% of the global population [@radage2023].
People rely on credit cards for everyday purchases, financial flexibility, and managing expenses. 
As credit card usage continues to grow, so does the risk of default, which is a significant concern for financial institutions. 
Credit card default occurs when a borrower fails to make their minimum payment obligations over an extended period of time [@streaks_nodate]. 
Understanding the factors that contribute to default can help banks and credit card companies make informed lending decisions and develop better risk assessment models. 
Therefore, our research question is:
 
> Will credit card customers default based on their demographic features, financial behaviors, and payment history?

The dataset used in this study is the Default of Credit Card Clients Dataset from the UCI Machine Learning Repository [@uci2016], which contains 30000 observations and 24 features, 
including information on credit limit, demographic factors, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. 
The target variable, `default.payment.next.month`, indicates whether a customer defaulted (`1`) or did not default (`0`).

::: {#tbl-predictors tbl-cap="Selected and mutated predictor variables used for analysis"}
| Predictor Variable | Description |
|--------------------|-------------|
| `CREDIT_LIMIT` | Amount of given credit in NT dollars |
| `SEX` | Gender (1 = male, 2 = female) |
| `EDUCATION` | Education level (1 = graduate school, 2 = university, 3 = high school, 4 = others, 5 = unknown, 6 = unknown) |
| `MARRIAGE` | Marital status (1 = married, 2 = single, 3 = others) |
| `AGE` | Age in years |
| `PAY_0` | Repayment status in September 2005 (-1 = pay duly, 1 = payment delay for one month, ..., 9 = payment delay for nine months or more) |
| `PAY_2` | Repayment status in August 2005 (same scale as above) |
| `PAY_3` | Repayment status in July 2005 (same scale as above) |
| `PAY_4` | Repayment status in June 2005 (same scale as above) |
| `PAY_5` | Repayment status in May 2005 (same scale as above) |
| `PAY_6` | Repayment status in April 2005 (same scale as above) |
| `AVG_BILL_AMT` | Average amount of bill statement from April to September 2005 (in NT dollars) |
| `AVG_PAY_AMT` | Average amount of previous payment from April to September 2005 (in NT dollars) |
:::

# Methods & Results

## Data Preprocessing

Cleaning & Handling missing values & Feature scaling & Train-test split.


```{r download-data, echo=FALSE, message=FALSE, warning=FALSE}

# Create raw data folder if it doesn't exist
if (!dir.exists("data/raw")) {
  dir.create("data/raw", recursive = TRUE)
}

# Define URL and destination path
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00350/UCI%20Credit%20Card.csv"
destfile <- "data/raw/UCI_Credit_Card.csv"

# Download if not already present
if (!file.exists(destfile)) {
  download.file(url, destfile, mode = "wb")
}
``` 


```{r tbl-data-preview, echo=FALSE, message=FALSE, warning=FALSE}
# Read the dataset
df <- read_csv("data/raw/UCI_Credit_Card.csv")
df <- clean_column_names(df)

# Show preview
df_preview <- head(df)


# Display the table with a caption and automatic numbering
knitr::kable(df_preview, caption = "Preview of the original dataset used in the analysis.")
```

@tbl-data-preview shows the first six rows of the dataset.


```{r echo=FALSE, message=FALSE, warning=FALSE}


# Split into training (80%) and testing (20%)
set.seed(123)  # Ensure reproducibility
splits <- split_data(df, train_size = 0.8)
train_df <- splits$train
test_df <- splits$test

# Convert to factor HERE once and for all
train_df$default_payment_next_month <- as.factor(train_df$default_payment_next_month)
test_df$default_payment_next_month <- as.factor(test_df$default_payment_next_month)

# Compute average columns (after factor conversion)
train_df <- compute_avg_amounts(train_df)
test_df <- compute_avg_amounts(test_df)

# Check the dimensions of training data
train_dim <- dim(train_df)

# Check class distribution
train_class_dist <- table(train_df$default_payment_next_month) / nrow(train_df)

# Summary statistics
train_summary <- summary(train_df)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Check for missing values
missing_values <- sum(is.na(train_df))
```


```{r tbl-cleaned-data, echo=FALSE, message=FALSE, warning=FALSE}


# Select first 6 rows for preview
train_df_preview <- head(train_df)

# Display formatted table with caption
kable(train_df_preview, caption = "Preview of the cleaned training dataset after feature engineering.")
```
@tbl-cleaned-data shows the cleaned training dataset with new aggregated features.


```{r tbl-final-cleaned-data, echo=FALSE, message=FALSE, warning=FALSE}
# Remove original BILL_AMT and PAY_AMT columns for a cleaner dataset
train_df <- train_df %>% select(-starts_with("BILL_AMT"), -starts_with("PAY_AMT"))
test_df <- test_df %>% select(-starts_with("BILL_AMT"), -starts_with("PAY_AMT"))

# Rename LIMIT_BAL to CREDIT_LIMIT for better readability
train_df <- rename(train_df, credit_limit = limit_bal)
test_df <- rename(test_df, credit_limit = limit_bal)

# Select first 6 rows for preview
train_df_final_preview <- head(train_df)

# Display formatted table with caption
kable(train_df_final_preview, caption = "Final cleaned training dataset after removing unnecessary columns and renaming features.")

```
@tbl-final-cleaned-data shows the final cleaned dataset after removing redundant columns and renaming features.

## Exploratory Data Analysis (EDA)

Visualization for distribution of target variable, brief view of relationship between target variable and exploratory variables.
	


```{r fig-target-dist, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Distribution of Default Payments", out.width="65%"}


# Distribution of default payments
knn_data <- prepare_knn_data(train_df, test_df, target = "default_payment_next_month")
make_target_plot(train_df)


```

@fig-target-dist shows the distribution of default payments, highlighting the imbalance in the dataset.

```{r fig-scatter-numeric, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Scatter plots of numeric features vs. default payment, showing general distribution with trend lines", out.width="80%"}
# Reshape the dataset for facet grid visualization
df_long <- train_df %>% 
  pivot_longer(cols = c(credit_limit, age, avg_bill_amt, avg_pay_amt, pay_0),
               names_to = "Feature", values_to = "Value")

# Scatter plots using facet grid with color for default class and trend lines
make_scatter_facet_plot(train_df, c("credit_limit", "age", "avg_bill_amt", "avg_pay_amt", "pay_0"))
```
    


@fig-scatter-numeric uses facet grid with color for default class and trend lines, showing all the general distribution
between numeric variables and the target variable

## Model Training

```{r fig-knn-accuracy, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="K-Value selection for KNN model, showing accuracy for different values of k"}

# Prepare KNN data
knn_data <- prepare_knn_data(train_df, test_df, target = "default_payment_next_month")

# Evaluate accuracy over multiple k-values
accuracy_results <- evaluate_k_values(knn_data$train, knn_data$test, knn_data$train_labels, knn_data$test_labels)
best_k <- accuracy_results$k[which.max(accuracy_results$accuracy)]

# Save accuracy plot
save_accuracy_plot(accuracy_results, "results/knn_accuracy.png")

make_accuracy_plot(accuracy_results)

# Train KNN model
knn_model <- class::knn(knn_data$train, knn_data$test, cl = knn_data$train_labels, k = best_k)

# Save confusion matrix and metrics
save_model_outputs(knn_model, knn_data$test_labels, "results/knn")
```

## Performance Evaluation

```{r tbl-conf-matrix, echo=FALSE, message=FALSE, warning=FALSE}
# Compute Accuracy, Precision, and Confusion Matrix
confusion <- confusionMatrix(as.factor(knn_model), as.factor(knn_data$test_labels))
false_negatives <- confusion$table[1, 2]
# Extract confusion matrix as a table
conf_matrix_table <- as.data.frame(confusion$table)

# Rename columns for better readability
colnames(conf_matrix_table) <- c("Predicted", "Actual", "Count")

# Display the table with a caption
knitr::kable(conf_matrix_table, caption = "Confusion Matrix for KNN Model Performance Evaluation.")
```
@tbl-conf-matrix presents the confusion matrix of the KNN model's performance.

```{r tbl-metrics, echo=FALSE, message=FALSE, warning=FALSE}
# Create a data frame with only values
metrics <- data.frame(
  Value = c(
    confusion$overall["Accuracy"],
    confusion$overall["AccuracyLower"],
    confusion$overall["AccuracyUpper"],
    confusion$overall["AccuracyNull"],
    confusion$overall["AccuracyPValue"],
    confusion$overall["Kappa"],
    confusion$overall["McnemarPValue"],
    confusion$byClass["Sensitivity"],
    confusion$byClass["Specificity"],
    confusion$byClass["Pos Pred Value"],
    confusion$byClass["Neg Pred Value"],
    confusion$byClass["Prevalence"],
    confusion$byClass["Detection Rate"],
    confusion$byClass["Detection Prevalence"],
    confusion$byClass["Balanced Accuracy"]
  )
)

# Assign row names separately
rownames(metrics) <- c(
  "Accuracy", "95% CI (Lower)", "95% CI (Upper)", "No Information Rate", 
  "P-Value (Acc > NIR)", "Kappa", "Mcnemar's Test P-Value",
  "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", 
  "Prevalence", "Detection Rate", "Detection Prevalence", "Balanced Accuracy"
)

# Display as a table without the first column
knitr::kable(metrics, caption = "Performance Metrics of KNN Model.")
```

@tbl-metrics summarizes the KNN model's performance metrics.

# Discussion

## Summary of Result:
The best value of *k* for the K-Nearest Neighbors (KNN) model was determined to be **`r best_k`**. @fig-knn-accuracy shows that as *k* increases, the accuracy stabilizes around 80%, indicating a good tradeoff between underfitting and overfitting.

From @tbl-metrics, our K-Nearest Neighbors (KNN) classification model achieved an accuracy of `r round(confusion$overall["Accuracy"] * 100, 2)`%, indicating that it correctly predicts default and non-default cases in about 80% of instances. 
However, the model performs significantly better at identifying non-defaulters (sensitivity: `r round(confusion$byClass["Sensitivity"] * 100, 2)`%) than defaulters (specificity: `r round(confusion$byClass["Specificity"] * 100, 2)`%). 
@tbl-conf-matrix reveals a high false negative rate (`r false_negatives` cases), meaning that many actual defaulters were misclassified as non-defaulters. 
The balanced accuracy of `r round(confusion$byClass["Balanced Accuracy"] * 100, 2)`% suggests that the model struggles with class imbalance, favoring the majority class (non-defaulters). 
To improve the result, we could use techniques to balance the dataset and try other classification models.

## Expectations:
These results are partially expected. Since credit card defaults are typically imbalanced datasets shown by @fig-target-dist, it was anticipated that the model might struggle with detecting defaulters. 
However, the low specificity was lower than expected, meaning the model fails to accurately identify defaulters more frequently than anticipated.

- We expected some bias toward the majority class (non-defaulters), but the high false negative rate suggests that KNN may not be the most effective method for this classification problem.

## Impact of Findings:
Ideally, the model can serve as a valuable tool for financial institutions in assessing credit risk and making informed lending decisions. Effectively distinguish between defaulters and non-defaulters, reducing financial losses by minimizing the risk of approving loans for high-risk individuals. However, in our model, the high false negative rate poses a risk to financial institutions, as they might approve loans for individuals who are likely to default. The low specificity could result in misleading credit scores, potentially granting loans to individuals who are unable to repay while unfairly rejecting creditworthy applicants. These findings highlight the need for improving predictive models. 

## Future Questions:

- How do external economic factors impact default rates?
    - Our model focuses on individual demographic and financial characteristics, but macroeconomic conditions such as inflation, interest rates, and unemployment rates could also play a crucial role in credit card default. Some existing studies have shown that “credit card default is not related to the amount of credit card customer’s income, but significantly to the stability of income” [@li2019]. Future studies could incorporate these external factors to improve predictive accuracy.


- Can other algorithms improve prediction accuracy?
    - While KNN provides useful insights, exploring other classification models such as logistic regression, decision trees, etc. may yield better performance. A comparative analysis could determine the most effective model for predicting defaults.


- How does financial behavior change over time, and can we predict long-term default risk?
    - Our model predicts default based on a fixed time period, but analyzing customer behavior over longer durations could reveal trends in financial stability. 


# References